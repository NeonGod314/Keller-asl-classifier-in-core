{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @author : Jimmy Fails\n",
    "\n",
    "## Training and testing for Android_ASL application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubhamkumar.singh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "from tf_util import random_mini_batches, convert_to_one_hot, predict, data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for classes\n",
    "dictn={'A':0,\n",
    "       'B':1,\n",
    "       'C':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 200, 200, 3)\n",
      "(86,)\n",
      "(172, 200, 200, 3)\n",
      "(172,)\n",
      "(258, 200, 200, 3)\n",
      "(258,)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# loading data for training\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "classes=['A','B','C']\n",
    "for clas in classes:\n",
    "    (x_train, y_train) = data_load('asl-alphabet/asl_alphabet_train/'+clas+'/',int(classes.index(clas)), x_train, y_train)\n",
    "    print(np.array(x_train).shape)\n",
    "    print(np.array(y_train).shape)\n",
    "print(y_train)    \n",
    "X_train_orig=np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428, 200, 200, 3)\n",
      "(428,)\n",
      "(468, 200, 200, 3)\n",
      "(468,)\n",
      "(896, 200, 200, 3)\n",
      "(896,)\n"
     ]
    }
   ],
   "source": [
    "# uploading the test data\n",
    "# loading data for training\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "classes=['A','B','C']\n",
    "for clas in classes:\n",
    "    (x_test, y_test) = data_load('asl-alphabet/asl_alphabet_test/'+clas+'/',int(classes.index(clas)), x_test, y_test)\n",
    "    print(np.array(x_test).shape)\n",
    "    print(np.array(y_test).shape)\n",
    "X_test_orig=np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output training and test variable processing\n",
    "Y_train_orig=np.array(y_train)\n",
    "Y_test_orig=np.array(y_test)\n",
    "Y_train_orig = Y_train_orig.reshape((1, Y_train_orig.shape[0]))\n",
    "Y_test_orig = Y_test_orig.reshape((1, Y_test_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing data into the suitable format\n",
    "x_train = np.array(x_train)\n",
    "x_train = np.squeeze(x_train)\n",
    "x_train = x_train.T\n",
    "x_test  = np.array(x_test)\n",
    "x_test  = np.squeeze(x_test)\n",
    "x_test  = x_test.T\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.reshape(1,9000)\n",
    "print(\"here\",y_test)\n",
    "y_test  = np.array(y_test)\n",
    "y_test  = y_test.reshape(1,3)\n",
    "\n",
    "print(\"Shape of input training data (ip_nodes, ip_imgs) :\",x_train.shape)\n",
    "print(\"Shape of input test data (ip_nodes, ip_imgs) :\",x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)\n",
    "print(y_train)\n",
    "#y_train = convert_to_one_hot(y_train, 3)\n",
    "#y_test  = convert_to_one_hot(y_test, 3)\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x,n_y):\n",
    "    X=tf.placeholder(tf.float32,[n_x,None], name=\"X\")\n",
    "    Y=tf.placeholder(tf.float32,[n_y,None], name=\"Y\")\n",
    "    return X, Y\n",
    "X, Y= create_placeholders(120000,3)\n",
    "print(\"X = \" + str(X))\n",
    "print(\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [25,120000], initializer=tf.contrib.layers.xavier_initializer(seed=1)) \n",
    "    b1 = tf.get_variable(\"b1\", [25,1],initializer=tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [12,25], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable(\"b2\", [12,1],initializer=tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [3,12], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable(\"b3\", [3,1],initializer=tf.zeros_initializer())\n",
    "    \n",
    "    parameters={\"W1\":W1,\n",
    "                \"b1\":b1,\n",
    "                \"W2\":W2,\n",
    "                \"b2\":b2,\n",
    "                \"W3\":W3,\n",
    "                \"b3\":b3}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)\n",
    "    \n",
    "    return Z3\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(120000, 3)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing cost or the loss function\n",
    "def compute_cost(Z3, Y):\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost   = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(120000, 3)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_epochs=1500, minibatch_size=32, print_cost=True):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    seed=3\n",
    "    \n",
    "    (n_x,m)= X_train.shape\n",
    "    n_y    = Y_train.shape[0]\n",
    "    costs  = []\n",
    "    \n",
    "    # creating placeholders\n",
    "    X, Y      = create_placeholders(n_x, n_y)\n",
    "    parameters= initialize_parameters()\n",
    "    Z3        = forward_propagation(X, parameters)\n",
    "    cost      = compute_cost(Z3, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # initializing all variables\n",
    "    init      = tf.global_variables_initializer()\n",
    "    \n",
    "    #starting the session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost=0.\n",
    "            num_minibatches=int(m/minibatch_size)\n",
    "            seed=seed+1\n",
    "            minibatches=random_mini_batches(X_train,Y_train,minibatch_size,seed)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                \n",
    "                (minibatch_X, minibatch_Y)=minibatch\n",
    "                _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y:minibatch_Y}) \n",
    "                \n",
    "                epoch_cost += minibatch_cost/num_minibatches\n",
    "                    \n",
    "        if print_cost == True and epoch % 100 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "        if print_cost == True and epoch % 5 == 0:\n",
    "            costs.append(epoch_cost)\n",
    "            \n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained\")\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
